{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4b7d86-d633-4f79-b96a-5fbace03e7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/son/Desktop/Research/simple-faster-rcnn-pytorch/model/utils/nms/non_maximum_suppression.py:12: UserWarning: \n",
      "    the python code for non_maximum_suppression is about 2x slow\n",
      "    It is strongly recommended to build cython code:\n",
      "    `cd model/utils/nms/; python3 build.py build_ext --inplace\n",
      "  warnings.warn('''\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as tvtsf\n",
    "from skimage import transform as sktsf\n",
    "from data.util import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from trainer import VictimFasterRCNNTrainer\n",
    "from utils import array_tool as at\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('TkAgg')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9756949-00e1-4037-945c-edcb7fcf6ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path, dtype=np.float32, color=True):\n",
    "    \"\"\"Read an image from a file.\n",
    "\n",
    "    This function reads an image from given file. The image is CHW format and\n",
    "    the range of its value is :math:`[0, 255]`. If :obj:`color = True`, the\n",
    "    order of the channels is RGB.\n",
    "\n",
    "    Args:\n",
    "        path (str): A path of image file.\n",
    "        dtype: The type of array. The default value is :obj:`~numpy.float32`.\n",
    "        color (bool): This option determines the number of channels.\n",
    "            If :obj:`True`, the number of channels is three. In this case,\n",
    "            the order of the channels is RGB. This is the default behaviour.\n",
    "            If :obj:`False`, this function returns a grayscale image.\n",
    "\n",
    "    Returns:\n",
    "        ~numpy.ndarray: An image.\n",
    "    \"\"\"\n",
    "\n",
    "    f = Image.open(path)\n",
    "    try:\n",
    "        if color:\n",
    "            img = f.convert('RGB')\n",
    "        else:\n",
    "            img = f.convert('P')\n",
    "        img = np.asarray(img, dtype=dtype)\n",
    "    finally:\n",
    "        if hasattr(f, 'close'):\n",
    "            f.close()\n",
    "\n",
    "    if img.ndim == 2:\n",
    "        # reshape (H, W) -> (1, H, W)\n",
    "        return img[np.newaxis]\n",
    "    else:\n",
    "        # transpose (H, W, C) -> (C, H, W)\n",
    "        return img.transpose((2, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3366e530-581a-4078-977d-8d34251ca9dd",
   "metadata": {},
   "source": [
    "# Dataset for WIDER FACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f438caf-cde5-41b8-b609-d1e6d866fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WIDERBboxDataset:\n",
    "    def __init__(self, path_to_label, path_to_image, fname):\n",
    "        self.path_to_label = path_to_label\n",
    "        self.path_to_image = path_to_image\n",
    "        self.f = scipy.io.loadmat(os.path.join(path_to_label, fname))\n",
    "        self.event_list = self.f.get('event_list')\n",
    "        self.file_list = self.f.get('file_list')\n",
    "        self.face_bbx_list = self.f.get('face_bbx_list')\n",
    "        self.label_names = WIDER_BBOX_LABEL_NAMES\n",
    "        self.im_list, self.bbox_list = self.get_img_list()\n",
    "        # ipdb.set_trace()\n",
    "        self.is_difficult = False\n",
    "\n",
    "    def get_img_list(self):\n",
    "        im_list = []\n",
    "        bbox_list = []\n",
    "        for event_idx, event in enumerate(self.event_list):\n",
    "            directory = event[0][0]\n",
    "            for im_idx, im in enumerate(self.file_list[event_idx][0]):\n",
    "                im_name = im[0][0]\n",
    "                im_list.append(os.path.join(self.path_to_image, directory,\\\n",
    "                        im_name + '.jpg'))\n",
    "                face_bbx = self.face_bbx_list[event_idx][0][im_idx][0]\n",
    "                bbox_list.append(face_bbx)\n",
    "        return im_list,bbox_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_list)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        \"\"\"Returns the i-th example.\n",
    "        Returns a color image and bounding boxes. The image is in CHW format.\n",
    "        The returned image is RGB.\n",
    "\n",
    "        Args:\n",
    "            i (int): The index of the example.\n",
    "\n",
    "        Returns:\n",
    "            tuple of an image and bounding boxes\n",
    "        \"\"\"\n",
    "        # Load a image\n",
    "        img_file = self.im_list[i]\n",
    "        face_bbx = self.bbox_list[i]\n",
    "        img = read_image(img_file, color=True)\n",
    "        bboxes = []\n",
    "        label = []\n",
    "        difficult = []\n",
    "        for i in range(face_bbx.shape[0]):\n",
    "            xmin = int(face_bbx[i][0])\n",
    "            ymin = int(face_bbx[i][1])\n",
    "            xmax = int(face_bbx[i][2]) + xmin\n",
    "            ymax = int(face_bbx[i][3]) + ymin\n",
    "            bboxes.append((ymin, xmin, ymax, xmax))\n",
    "            label.append(WIDER_BBOX_LABEL_NAMES.index('Face'))\n",
    "            difficult.append(self.is_difficult)\n",
    "        bboxes = np.stack(bboxes).astype(np.float32)\n",
    "        label = np.stack(label).astype(np.int32)\n",
    "        difficult = np.array(difficult, dtype=np.bool_).astype(np.uint8)  # PyTorch don't support np.bool\n",
    "        return img, bboxes, label, difficult\n",
    "\n",
    "    __getitem__ = get_example\n",
    "\n",
    "\n",
    "WIDER_BBOX_LABEL_NAMES = (\n",
    "    'Face')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4fd6264-95da-4fed-bb33-95d936769aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.WIDERBboxDataset at 0x7f44884b6490>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wider_label_dir = '/home/son/Desktop/Research/simple-faster-rcnn-pytorch/data/wider_face_split'\n",
    "wider_data_dir = '/home/son/Desktop/Research/simple-faster-rcnn-pytorch/data/WIDER_train/images'\n",
    "wider_fname_mat = 'wider_face_train'\n",
    "\n",
    "# Dataset\n",
    "db = WIDERBboxDataset(wider_label_dir, wider_data_dir, wider_fname_mat)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc811a4b-1520-48b5-8a8b-76740c932f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(db.get_example(1)[0].transpose(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f2457-5484-4859-bf09-e47fff004205",
   "metadata": {},
   "source": [
    "# Wrapper Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "462735e3-06d9-4288-969a-f913990594be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_normalize(img):\n",
    "    \"\"\"\n",
    "    https://github.com/pytorch/vision/issues/223\n",
    "    return appr -1~1 RGB\n",
    "    \"\"\"\n",
    "    # normalize = tvtsf.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "    #                             std=[0.229, 0.224, 0.225])\n",
    "    normalize = tvtsf.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                std=[0.5, 0.5, 0.5])\n",
    "    img = normalize(t.from_numpy(img))\n",
    "    return img.numpy()\n",
    "\n",
    "def preprocess(img, min_size=600, max_size=1000):\n",
    "    \"\"\"Preprocess an image for feature extraction.\n",
    "\n",
    "    The length of the shorter edge is scaled to :obj:`self.min_size`.\n",
    "    After the scaling, if the length of the longer edge is longer than\n",
    "    :param min_size:\n",
    "    :obj:`self.max_size`, the image is scaled to fit the longer edge\n",
    "    to :obj:`self.max_size`.\n",
    "\n",
    "    After resizing the image, the image is subtracted by a mean image value\n",
    "    :obj:`self.mean`.\n",
    "\n",
    "    Args:\n",
    "        img (~numpy.ndarray): An image. This is in CHW and RGB format.\n",
    "            The range of its value is :math:`[0, 255]`.\n",
    "         (~numpy.ndarray): An image. This is in CHW and RGB format.\n",
    "            The range of its value is :math:`[0, 255]`.\n",
    "\n",
    "    Returns:\n",
    "        ~numpy.ndarray:\n",
    "        A preprocessed image.\n",
    "\n",
    "    \"\"\"\n",
    "    C, H, W = img.shape\n",
    "    scale1 = min_size / min(H, W)\n",
    "    scale2 = max_size / max(H, W)\n",
    "    scale = min(scale1, scale2)\n",
    "    img = img / 255.\n",
    "\n",
    "    img = sktsf.resize(img, (C, H * scale, W * scale), mode='reflect')\n",
    "    # both the longer and shorter should be less than\n",
    "    # max_size and min_size\n",
    "\n",
    "    # Only using pytorch normalize\n",
    "    normalize = pytorch_normalize\n",
    "    return normalize(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829402eb-9628-4509-8f3c-c773f8e0bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform(object):\n",
    "    def __init__(self, min_size=600, max_size=1000):\n",
    "        self.min_size = min_size\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, in_data):\n",
    "        img, bbox, label = in_data\n",
    "        _, H, W = img.shape\n",
    "        img = preprocess(img, self.min_size, self.max_size)\n",
    "        _, o_H, o_W = img.shape\n",
    "        scale = o_H / H\n",
    "        bbox = resize_bbox(bbox, (H, W), (o_H, o_W))\n",
    "\n",
    "        # # horizontally flip\n",
    "        img, params = random_flip(img, x_random=True, return_param=True)\n",
    "        bbox = flip_bbox(bbox, (o_H, o_W), x_flip=params['x_flip'])\n",
    "\n",
    "        return img, bbox, label, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40c2a8e8-860f-4e31-9755-aadc78acdca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for wider face\n",
    "class Dataset:\n",
    "    def __init__(self, label_dir, data_dir, fname_mat, min_size=600, max_size=1000):\n",
    "        self.db = WIDERBboxDataset(label_dir, data_dir, fname_mat)\n",
    "        self.tsf = Transform(min_size, max_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ori_img, bbox, label, difficult = self.db.get_example(idx)\n",
    "\n",
    "        img, bbox, label, scale = self.tsf((ori_img, bbox, label))\n",
    "        # TODO: check whose stride is negative to fix this instead copy all\n",
    "        # some of the strides of a given numpy array are negative.\n",
    "        return img.copy(), bbox.copy(), label.copy(), scale\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40038222-e20b-4a2d-8946-f3c76a5bc459",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23f360f3-ef19-4abd-b2b0-4c30e644fc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f43621053d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils import data as data_\n",
    "\n",
    "# opt\n",
    "wider_label_dir = '/home/son/Desktop/Research/simple-faster-rcnn-pytorch/data/wider_face_split'\n",
    "wider_data_dir = '/home/son/Desktop/Research/simple-faster-rcnn-pytorch/data/WIDER_train/images'\n",
    "wider_fname_mat = 'wider_face_train'\n",
    "num_workers = 1\n",
    "\n",
    "dataset = Dataset(wider_label_dir, wider_data_dir, wider_fname_mat)\n",
    "dataloader = data_.DataLoader(dataset, \\\n",
    "                              batch_size=1, \\\n",
    "                              shuffle=True, \\\n",
    "                              pin_memory=True,\\\n",
    "                              num_workers=num_workers)\n",
    "dataloader # img, bbox, label, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca2f196b-76f4-4fcc-ad59-9df1c18ae69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inverse_normalize(img):\n",
    "#     return (img * 0.5 + 0.5).clip(min=0, max=1) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5531894-7c85-4e99-85f1-ed9de6a5264d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(840, 600, 3)\n"
     ]
    }
   ],
   "source": [
    "x = next(iter(dataloader))\n",
    "# x = np.int32(inverse_normalize(at.tonumpy(x[0].squeeze(0))))\n",
    "print(x[0].squeeze(0).numpy().transpose(1, 2, 0).shape)\n",
    "plt.imshow(x[0].squeeze(0).numpy().transpose(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a4e1b7-989d-4ab6-9638-37e084caf505",
   "metadata": {},
   "source": [
    "# FasterRCNNVGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8702dca4-d81b-4f09-a8db-fb3f3fe63fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNNVGG16(\n",
       "  (extractor): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (score): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (loc): Conv2d(512, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (head): VGG16RoIHead(\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (cls_loc): Linear(in_features=4096, out_features=8, bias=True)\n",
       "    (score): Linear(in_features=4096, out_features=2, bias=True)\n",
       "    (roi): RoIPooling2D()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.faster_rcnn_vgg16 import FasterRCNNVGG16\n",
    "\n",
    "faster_rcnn = FasterRCNNVGG16()\n",
    "faster_rcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2546ff-b2ff-4cc1-ae7d-18230e212dab",
   "metadata": {},
   "source": [
    "# DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "980afe98-121a-4438-91df-0aca30cbdfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(nn.Module):\n",
    "    def __init__(self, num_channels=3, ngf=100, cg=0.05, learning_rate=1e-4, train_adv=False):\n",
    "        \"\"\"\n",
    "\t\tInitialize a DCGAN. Perturbations from the GAN are added to the inputs to\n",
    "\t\tcreate adversarial attacks.\n",
    "\n",
    "\t\t- num_channels is the number of channels in the input\n",
    "\t\t- ngf is size of the conv layers\n",
    "\t\t- cg is the normalization constant for perturbation (higher means encourage smaller perturbation)\n",
    "\t\t- learning_rate is learning rate for generator optimizer\n",
    "\t\t- train_adv is whether the model being attacked should be trained adversarially\n",
    "\t\t\"\"\"\n",
    "        super(DCGAN, self).__init__()\n",
    "        self.generator = nn.Sequential(\n",
    "            # 3 x 32 x 32\n",
    "            nn.Conv2d(num_channels, ngf, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 1000 x 32 x 32\n",
    "            nn.Conv2d(ngf, ngf, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 1000 x 32 x 32\n",
    "            nn.Conv2d(ngf, ngf, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 1000 x 32 x 32\n",
    "            nn.Conv2d(ngf, ngf, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 1000 x 32 x 32\n",
    "            nn.Conv2d(ngf, ngf, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 1000 x 32 x 32\n",
    "            nn.Conv2d(ngf, ngf, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 1000 x 32 x 32\n",
    "            nn.Conv2d(ngf, ngf, 3, 1, 1, bias=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 3 x 32 x32\n",
    "            nn.Conv2d(ngf, num_channels, 1, 1, 0, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.cuda = t.cuda.is_available()\n",
    "\n",
    "        if self.cuda:\n",
    "            self.generator.cuda()\n",
    "            cudnn.benchmark = True\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "        self.cg = cg\n",
    "        self.optimizer = optim.Adam(self.generator.parameters(), lr=learning_rate)\n",
    "        self.train_adv = train_adv\n",
    "        self.max_iter = 20\n",
    "        self.c_misclassify = 1\n",
    "        self.confidence = 0\n",
    "\n",
    "    def forward(self, inputs, model, labels=None, bboxes=None, scale=None,\\\n",
    "                model_feats=None, model_optimizer=None, *args):\n",
    "        num_unperturbed = 10 # cái này là cái gì???\n",
    "        iter_cnt = 0\n",
    "        loss_perturb = 20\n",
    "        loss_misclassify = 10\n",
    "\n",
    "        while loss_misclassify > 0 and loss_perturb > 1:\n",
    "            perturbation = self.generator(inputs)\n",
    "            adv_inputs = inputs + perturbation\n",
    "            adv_inputs = torch.clamp(adv_inputs, -1.0, 1.0) # giới hạn các pixel trong khoảng -1 dến 1\n",
    "            scores, gt_labels = model(adv_inputs, bboxes, labels, scale, attack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4280f2d7-9321-4501-9a5d-b2cb68c1e007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DCGAN(\n",
       "  (generator): Sequential(\n",
       "    (0): Conv2d(3, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (4): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (6): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (10): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (12): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (14): Conv2d(100, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (15): Tanh()\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attacker = DCGAN(train_adv=False)\n",
    "attacker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e014c70c-2bc5-43b1-ad5e-d2c9689bdf84",
   "metadata": {},
   "source": [
    "# VictimFasterRCNN Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c30b8c-1ea1-43a7-97db-fc0d9e818a8a",
   "metadata": {},
   "source": [
    "# Train VictimFasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfec4690-5b9f-49eb-880e-670e466354a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VictimFasterRCNNTrainer(\n",
       "  (faster_rcnn): FasterRCNNVGG16(\n",
       "    (extractor): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "    )\n",
       "    (rpn): RegionProposalNetwork(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (score): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (loc): Conv2d(512, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (head): VGG16RoIHead(\n",
       "      (classifier): Sequential(\n",
       "        (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "      (cls_loc): Linear(in_features=4096, out_features=8, bias=True)\n",
       "      (score): Linear(in_features=4096, out_features=2, bias=True)\n",
       "      (roi): RoIPooling2D()\n",
       "    )\n",
       "  )\n",
       "  (attacker): DCGAN(\n",
       "    (generator): Sequential(\n",
       "      (0): Conv2d(3, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (2): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (4): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (6): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (8): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (10): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (12): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (14): Conv2d(100, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (15): Tanh()\n",
       "    )\n",
       "    (criterion): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15d50129-3abd-4642-8afd-f4dd3cfbb6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_normalize(img):\n",
    "    return (img * 0.5 + 0.5).clip(min=0, max=1) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7589f38-d310-47e5-898e-b039188e5403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.vis_tool import visdom_bbox\n",
    "\n",
    "trainer = VictimFasterRCNNTrainer(faster_rcnn, attacker).cuda()\n",
    "trainer.vis.text(dataset.db.label_names, win='labels')\n",
    "\n",
    "for epoch in range(20):\n",
    "    trainer.reset_meters()\n",
    "    for ii, (img, bbox_, label_, scale) in tqdm(enumerate(dataloader)):\n",
    "        scale = at.scalar(scale)\n",
    "        img, bbox, label = img.cuda().float(), bbox_.cuda(), label_.cuda()\n",
    "        img, bbox, label = Variable(img), Variable(bbox), Variable(label)\n",
    "        adv_inputs = trainer.train_step(img, bbox, label, scale)\n",
    "        print(adv_inputs)\n",
    "        \n",
    "        ori_img_ = inverse_normalize(at.tonumpy(img[0]))\n",
    "        gt_img = visdom_bbox(ori_img_,\n",
    "                             at.tonumpy(bbox_[0]),\n",
    "                             at.tonumpy(label_[0]))\n",
    "        trainer.vis.img('gt_img', gt_img)\n",
    "        _bboxes, _labels, _scores = trainer.faster_rcnn.predict([ori_img_], visualize=True)\n",
    "        pred_img = visdom_bbox(ori_img_,\n",
    "                               at.tonumpy(_bboxes[0]),\n",
    "                               at.tonumpy(_labels[0]).reshape(-1),\n",
    "                               at.tonumpy(_scores[0]))\n",
    "        trainer.vis.img('pred_img', pred_img)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c280d8b8-482f-4bcc-a8e2-b70e9f79b534",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mopt\u001b[49m\u001b[38;5;241m.\u001b[39mepoch):\n\u001b[1;32m      2\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mreset_meters()\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ii, (img, bbox_, label_, scale) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(dataloader)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(opt.epoch):\n",
    "    trainer.reset_meters()\n",
    "    for ii, (img, bbox_, label_, scale) in tqdm(enumerate(dataloader)):\n",
    "        scale = at.scalar(scale)\n",
    "        img, bbox, label = img.cuda().float(), bbox_.cuda(), label_.cuda()\n",
    "        img, bbox, label = Variable(img), Variable(bbox), Variable(label)\n",
    "        losses,adv_inputs = trainer.train_step(img, bbox, label, scale)\n",
    "\n",
    "        if (ii + 1) % opt.plot_every == 0:\n",
    "            if os.path.exists(opt.debug_file):\n",
    "                ipdb.set_trace()\n",
    "\n",
    "            # plot loss\n",
    "            trainer.vis.plot_many(trainer.get_meter_data())\n",
    "\n",
    "            # plot groud truth bboxes\n",
    "            ori_img_ = inverse_normalize(at.tonumpy(img[0]))\n",
    "            gt_img = visdom_bbox(ori_img_,\n",
    "                                 at.tonumpy(bbox_[0]),\n",
    "                                 at.tonumpy(label_[0]))\n",
    "            trainer.vis.img('gt_img', gt_img)\n",
    "\n",
    "            # plot predicti bboxes\n",
    "            _bboxes, _labels, _scores = trainer.faster_rcnn.predict([ori_img_], visualize=True)\n",
    "            pred_img = visdom_bbox(ori_img_,\n",
    "                                   at.tonumpy(_bboxes[0]),\n",
    "                                   at.tonumpy(_labels[0]).reshape(-1),\n",
    "                                   at.tonumpy(_scores[0]))\n",
    "            trainer.vis.img('pred_img', pred_img)\n",
    "            _bboxes, _labels, _scores = trainer.faster_rcnn.predict([adv_inputs], visualize=True)\n",
    "            pred_img = visdom_bbox(ori_img_,\n",
    "                                   at.tonumpy(_bboxes[0]),\n",
    "                                   at.tonumpy(_labels[0]).reshape(-1),\n",
    "                                   at.tonumpy(_scores[0]))\n",
    "            trainer.vis.img('adv_img', pred_img)\n",
    "\n",
    "            # rpn confusion matrix(meter)\n",
    "            trainer.vis.text(str(trainer.rpn_cm.value().tolist()), win='rpn_cm')\n",
    "            # roi confusion matrix\n",
    "            trainer.vis.img('roi_cm', at.totensor(trainer.roi_cm.conf, False).float())\n",
    "\n",
    "    #eval_result = eval(test_dataloader, faster_rcnn, test_num=2000)\n",
    "    # if eval_result['map'] > best_map:\n",
    "        # best_map = eval_result['map']\n",
    "        # best_path = trainer.save(best_map=best_map)\n",
    "    # if epoch == 9:\n",
    "        # trainer.load(best_path)\n",
    "        # trainer.faster_rcnn.scale_lr(opt.lr_decay)\n",
    "\n",
    "    # trainer.vis.plot('test_map', eval_result['map'])\n",
    "    # lr_ = trainer.faster_rcnn.optimizer.param_groups[0]['lr']\n",
    "    # log_info = 'lr:{}, map:{},loss:{}'.format(str(lr_),\n",
    "                                              # str(eval_result['map']),\n",
    "                                              # str(trainer.get_meter_data()))\n",
    "    # trainer.vis.log(log_info)\n",
    "    if epoch == 13:\n",
    "        best_path = trainer.save(epochs=epoch)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
